{
  "model_type": "lstm",
  "model_size": "tiny",
  "n_layer": 2,
  "n_embd": 256,
  "hidden_size": 256,
  "vocab_size": 37,
  "block_size": 64,
  "dropout": 0.1,
  "batch_size": 256,
  "learning_rate": 0.0003,
  "weight_decay": 0.01,
  "beta1": 0.9,
  "beta2": 0.999,
  "grad_clip": 1.0
}
